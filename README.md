# Disaster Tweets Classification

This project aims to build a machine learning model that predicts which Tweets are about real disasters and which ones aren't. It uses a dataset of 10,000 tweets and explores different architectures like LSTM and GRU to identify the best-suited model for the task.

## Project Structure

* **disaster_tweets.ipynb:** Jupyter Notebook containing the code for data loading, preprocessing, model building, training, and evaluation.
* **submission_lstm.csv:** Submission file generated by the Bidirectional LSTM model.
* **submission_gru.csv:** Submission file generated by the Bidirectional 2xGRU model.
* **README.md:** This file.


## Dataset

The dataset used in this project is the "Natural Language Processing with Disaster Tweets" dataset from Kaggle. It consists of 10,000 tweets labeled as either disaster-related (1) or not (0).

## Data Preprocessing

The following preprocessing steps are applied to the text data:

* Converting text to lowercase
* Removing punctuation and additional empty strings
* Stemming using PorterStemmer
* Removing stop words


## Model Building and Training

Two different architectures are explored:

* **Bidirectional LSTM:** A recurrent neural network with bidirectional LSTM layers.
* **Bidirectional 2xGRU:** A recurrent neural network with two bidirectional GRU layers.

The models are trained using the `rmsprop` optimizer and evaluated using accuracy as the metric. Early stopping and learning rate reduction are implemented to prevent overfitting and improve performance.

## Results

The Bidirectional LSTM model outperformed the Bidirectional GRU model with a test accuracy of 0.797 vs 0.644. While GRUs are generally faster, in this case, LSTM showed better generalization on validation and test sets.

## Future Work

For future work, exploring transformer-based models like BERT could further improve performance. Fine-tuning hyperparameters and experimenting with different text preprocessing techniques could also lead to better results.

## Usage

To run the project, you need to have Jupyter Notebook installed along with the necessary libraries such as pandas, scikit-learn, nltk, tensorflow, etc. 

1. Download the dataset from Kaggle and place it in the `input` folder.
2. Open the `disaster_tweets.ipynb` notebook and run the cells.
3. The notebook will generate two submission files, `submission_lstm.csv` and `submission_gru.csv`.
